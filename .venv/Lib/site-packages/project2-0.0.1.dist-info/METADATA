Metadata-Version: 2.4
Name: project2
Version: 0.0.1
Summary: A parser for the BYU CS 236 Datalog interpreter
Requires-Python: >=3.11
Description-Content-Type: text/markdown
Provides-Extra: dev
Requires-Dist: pytest; extra == "dev"
Requires-Dist: mypy; extra == "dev"
Requires-Dist: ruff; extra == "dev"
Requires-Dist: pre-commit; extra == "dev"
Requires-Dist: ipykernel; extra == "dev"
Requires-Dist: ipytest; extra == "dev"
Provides-Extra: classroom
Requires-Dist: pytest; extra == "classroom"

# Project 2

This project uses the `lexer` function from Project 1. That function takes as input a Datalog program as a string, and it returns an iterator to a `Token`. Each token generated by the iterator is a syntactic element of the Datalog program input. Project 2 implements a **parser** that processes the tokens from the iterator to accomplish two things:

  1. Determine if the input Datalog program is syntactically correct. In other words, determine whether the sequence of tokens, which are the syntactic elements of the input, represents a valid Datalog program.
  2. If the Datalog program is syntactically correct, build an instance of a `DatalogProgram` object from the observed sequence of tokens. The `DatalogProgram` has a special structure, described later, that will be used as the input to Project 3.

You will implement two functions in the `parser.py` module: `parser.parse` and `parser.datalog`. The input to the `parser.parse` function is the token iterator generated in Project 1 when the `lexer` function operates on some input, and the output from the `parser.parse` function is an instance of the `DatalogProgram` class iff the input is syntactically correct; otherwise it raises an `UnexpectedTokenException`. The `parser.parse` function must implement a predictive top-down parsing algorithm that is recursive.

The predictive top-down parsing algorithm is derived from the Datalog grammar using _FIRST_ and _FOLLOW_ sets for each of the rules in the grammar. The mathematical definition of a grammar, grammar rules, FIRST, FOLLOW, etc. are in the lecture notes on [learningsuite.byu.edu](https://learningsuite.byu.edu) in the _Content_ section. Creating a top-down predictive parsing algorithm from those things is also in the lecture notes. Further, we provide an extensive Jupyter notebook that walks through implementing the `parse` function on a simple example.

**Summary of Documentation**
- [README.md](README.md): describes project logistics
- [PARSER.md](docs/PARSER.md): describes the parser and Datalog grammar
- [CODE.md](docs/CODE.md): describes the starter code
- [Recursive descent parsing with FIRST sets](docs/Recursive_descent_parsing_code_example_2024.ipynb) (Jupyter Notebook)
- [Recursive descent parsing with FOLLOW sets](docs/Recursive_descent_parsing_code_with_FOLLOW_example_2024.ipynb) (Jupyter Notebook)
- Lecture notes in [learningsuite.byu.edu](https://learningsuite.byu.edu) including slides on _Building your tests for Project 2_.

## Table of Contents

- [Developer Setup](#developer-setup)
- [Project Requirements](#project-requirements)
- [AI Policy](#ai-policy-for-project-2)
- [Unit Tests](#unit-tests)
- [Integration Tests (pass-off)](#integration-tests-pass-off)
- [Code Quality Tools](#code-quality-tools)
- [Submission and Grading](#submission-and-grading)
- [Best Practices](#best-practices)

## Developer Setup

The `vscode` extensions for developing Project 2 are already installed as part of Project 0. You should not need to install any new extensions. You do need to set up the project locally on your machine. The below steps outline the process.

1. Clone the repository to your machine. Accepting the Project 2 assignment on GitHub classroom creates a repository for your submission. You need to clone that repository to your machine. Copy the URL generated after accepting the assignment and in a terminal on your machine in a sensible location. From an integrated terminal, type `git clone \<URL\>` where `\<URL\>` is the one you copied. Or open a new vscode window, select _Clone Git Repository_, and paste the URL you copied. If you followed the URL to GitHub, then you can recopy the URL using the "<> Code â–¼" button.
1. Create and activate a virtual environment in the project directory.  Revisit Project 0 for a reminder on how to create the virtual environment. There is also a _cheat sheet_ at [learningsuite.byu.edu](https://learningsuite.byu.edu) _Content_ &rarr; _Projects_ &rarr; _Projects Cheat Sheet_.
1. Install the project package. **Be sure your virtual environment is active before installing the package!** In a terminal in the virtual environment in the project directory do: `pip install --editable ".[dev]"`. Use `pip3` instead of `pip` if your system requires it.
1. Verify the package installation. From the terminal in which you activated the virtual environment and installed the project package, type `project2` and hit enter. You should see the below output.
    ```
    $ project2
    usage: project2 <input file>
    ```
1. Install `pre-commit` for the project. **Be sure your virtual environment is active before installing pre-commit!**. In a terminal in the virtual environment in the project directory do: `pre-commit install`
1. Verify the `pre-commit` installation. From the terminal in which you installed `pre-commit`, type `pre-commit run --all-files` and hit enter. You should see something like the below at the end of the setup output:
    ```
    $ pre-commit run --all-files
    trim trailing whitespace.................................................Passed
    fix end of files.........................................................Passed
    check yaml...............................................................Passed
    check for added large files..............................................Passed
    CRLF end-lines remover...................................................Passed
    Tabs remover.............................................................Passed
    ruff.....................................................................Passed
    ruff-format..............................................................Passed
    mypy.....................................................................Passed
    ```
1. Verify that tests are ready to run. Open the _Testing Pane_ in VS Code by clicking on the test tube icon. If you see
    ```
    pytest Discovery Error [project-2]
    ```
    then you must open the _Command Palette_ from the _View_ menu, choose `Python: Select Interpreter`, and choose the interpreter for the virtual environment (probably `Python 3.12.5 (.venv)`).
1. **IMPORTANT**: Copy the below files from your solution to Project 1 into the `src/project2/` folder:
    * `fsm.py`
    * `lexer.py`
  The `token.py` file is unchanged here and should not be copied over. None of test files from Project 1 should be copied over either.
1. **IMPORTANT**: Comments are not used to generate the `DatalogProgram` in Project 2, so you will modify `lexer.py` to ignore comments. Edit `lexer.py` and add `COMMENT` to the list of hidden tokens. So `WHITESPACE` and `COMMENT` should both be hidden and not passed to the parser.

## Project Requirements

1. The project must be completed individually -- there is no group work.
1. Project pass-off is on GitHub. You will commit your final solution to the `master` branch of your local repository and then push that commit to GitHub. Multiple commits, and pushes, are allowed. A push triggers a GitHub action that is the auto-grader for pass-off. The TAs look at both the result of the auto-grader on GitHub and your code to determine your final score. Projects that use iteration instead of tail recursion will not be accepted.
1. You must pass all integration tests up to, and including, `tests/test_passoff_80.py` to move on to the next project. Bucket 80 is the minimum functionality to complete the course.
1. You must implement the `__str__` method for the `DatalogProgram` class -- see [PARSER.md](docs/PARSER.md).
1. You must implement a deterministic top-down parser that chooses which production to use based on the current token.
    - You must have a function for each non-terminal.
    - You must implement a recursive-descent parser using `FIRST` sets.
    - You must implement tail recursion for the list-generating productions like `schemelist` and `idlist` using `FOLLOW` sets to terminate the recursion. You are not allowed to use any while loops in list productions. **WARNING**: a naive application of an AI tool will produce code that will use a while loop.
1. You must use the lexer from the previous project to read tokens from the input.
1. You must use the classes for `DatalogProgram`, `Rule`, `Predicate`, and `Parameter` provided in the starter code.
1. Your code must not report any issues with the following code quality tool run in the integrated `vscode` terminal from the root of the project directory: `pre-commit run --all-files`. This tool includes _type checking_ so your solution requires type annotations.

Consider using a branch as you work on your submission so that you can `commit` your work from time to time. Once everything is working, and the auto-grader tests are passing, then you can `merge` your work into your master branch and push it to your GitHub repository. Ask your favorite AI for help learning how to use Git branches for feature development.

## AI Policy for Project 2
The AI policy for this project tries to balance three objectives:
- Help you learn how to write code for a a recursive descent parser that uses the mathematics of `FIRST` and `FOLLOW` sets
- Help you improve your judgment about when to use AI tools in your programming
- Avoid busy work

You will write a function for each production in [PARSER.md](docs/PARSER.md). The productions for the non-terminals that end with _"List"_ require tail recursion and `FOLLOW` sets: `schemeList`, `idList`, etc. The productions for the other non-terminals require only `FIRST` sets.

The AI Policy for this Project is
1. Do not use Copilot-based autocomplete for writing functions in VS Code
2. Write **both the functions and the tests** for the productions that use the following non-terminals **by hand** (i.e., without AI assistance):
  - Those that use only `FIRST` sets:
     - `datalogProgram`
     - `scheme`
     - `rule`
     - `headPredicate`
  - Those that use `FOLLOW` sets:
    - `schemeList`
    - `factList`
    - `predicateList`
    - `idList`
3. You may use AI tools to write both the functions and the tests for the other non-terminals given the following conditions:
  - The prompt you use to generate code must include example code from one of the functions you have written with instructions to the AI tool to use that pattern
  - The prompt you use to generate tests must be based on a portion of a parse tree that you have solved by hand
  - You understand and test the code written by AI before you ask for TA help and before you submit the project

Ethically using AI requires proper attribution of where and how you used AI. You will need to describe and justify the AI prompts you used when you submit your project on Learning Suite.

## The TokenStream

We provide a **wrapper** around the `Iterator[Token]` returned from your `lexer` function in Project 1. For the purposes of this lab, think of a "wrapper" as a **helper class** that sits on top of the `Iterator[token]` returned from the lexer. This helper class manages the token iterator.

The wrapper is designed to make implementing recursive descent parsing with `FIRST` and `FOLLOW` sets easy in regards to matching tokens and advancing to the next token from the lexer.  [parser.py in CODE.md](docs/CODE.md#parserpy) provides a detailed explanation of the `TokenStream` class. `TokenStream` is defined in `src/project2/parser.py` and its definition includes detailed docstrings about its usage. **You are strongly encouraged to read, and be sure you understand, the `TokenStream` write up and docstrings before starting the project.**

## Unit Tests

As stated before, testing will be done through Github Classroom. You can also test your code locally with your own input, and by running the test cases in `vscode`. If you forgot how to do this, review the Project 0 help video here: https://www.youtube.com/watch?v=jZOf5oN-lKA. Just like Project 0, your grade will be whatever grade is on Github Classroom when you submit your code.

The `tests/test_parser.py` file does include a _docstring_ at the end of the file showing an example for how you might consider unit testing your parser as you develop. _Test driven development_ (TDD) is a development mindset driven by tests. In general, you write a test for the new functionality, verify the test fails, implement the functionality, and then verify the test passes. TDD is an effective approach to breaking a problem into small manageable pieces.

See the [AI Policy](#ai-policy-for-project-2) for instructions on when to use and not use AI in generating tests.

## Integration Tests (pass-off)

The primary tests are found in the `tests/test_passoff_xx.py` files. These tests are used for project pass-off. The `xx` on each bucket denotes the available points for passing the tests in that bucket. The value of each test in each bucket is uniform: _points-for-bucket/number-of-tests-in-bucket_. Bucket 80 is the minimum requirement to pass the course.

## Code Quality Tools (pre-commit)

One of the goals of this class is to help you learn more powerful programming tools than you've used in previous classes. The `pre-commit` tool is an extremely useful tool for checking code quality _before_ before committing into a repository. It works in conjunction with `git commit` using a mechanism called a _hook_. Git provides a way to run code before, and after, certain `git` actions. The `pre-commit install` completed during the [Developer Setup](#developer-setup) installed a hook to run `pre-commit` on staged files on `commit` actions. The result is that anytime you issue a `git commit`, any file included in that commit in checked by `pre-commit`. This behavior can be overridden with the `--no-verify` flag when added at the end of the `git commit` command. Overriding the `pre-commit` hook for `git` is **strongly discouraged** especially since you are required to show that `pre-commit` runs with no warnings or errors as part of the [Project Requirements](#project-requirements).

What does `pre-commit` check? The `pre-commit` checks are defined as hooks in the `.pre-commit-config.yaml` file. That file defines the following hooks:
1. (https://github.com/pre-commit/pre-commit-hooks)[https://github.com/pre-commit/pre-commit-hooks] for
    * id: `trailing-whitespace`: removes trailing whitespace on any line.
    * id: `end-of-file-fixer`: makes sure all non-binary files end with a newline.
    * id: `check-yaml`: checks syntax on yaml files.
    * id: `check-added-large-files`: blocks adding large files.
1. [https://github.com/Lucas-C/pre-commit-hooks](https://github.com/Lucas-C/pre-commit-hooks) for
    * id: `remove-crlf`: converts all end-of-line encodings to `lf`
    * id: `remove-tabs`: converts all tabs to spaces.
1. [https://github.com/astral-sh/ruff-pre-commit](https://github.com/astral-sh/ruff-pre-commit) for
    * id: `ruff`: check for code smells.
    * id: `ruff-format`: format all code.
1. [https://github.com/pre-commit/mirrors-mypy](https://github.com/pre-commit/mirrors-mypy) for
    * id: `mypy`

The _id_ annotations in the above are the names of the hooks. You can use the names if you want to run a particular hook on a file:

```
$ pre-commit run mypy --files src/project2/lexer.py
mypy.....................................................................Passed
```

The above runs the `mypy` check on `src/project2/lexer.py`. You can give a list of files with the `--files` flag, or you can just run all files known to the repository with `--all-files`.

When a file fails a check, `pre-commit` reports the failure with any needed details to correct it. It automatically modifies files where possible. Modified files will need to be re-added to the `git` index when `pre-commit` fails during `git commit` otherwise you will git the same error reported. Remember, `git commit` only looks at files in the index, so if a file is in the index, and `pre-commit` modifies that file, then a `git status` will show the file in the index **and** the same file as being modified. A `git add <file>` will add the new modified version of the file from `pre-commit` into the index to be committed.

## Submission and Grading

The minimum standard for this project is **bucket 80**. That means that if all the tests pass in all buckets up to and including bucket 80, then the next project can be started safely. You can run each bucket from the testing pane or with `pytest` on the command line. Passing everything up to and including `test_passoff_80.py` is the minimum requirement to move on to the next project.

Submit Project 2 for grading by doing the following:

  * Commit your solution on the master branch
  * Push the commit to GitHub -- that should trigger the auto-grader
  * Goto [learningsuite.byu.edu](https://learningsuite.byu.edu) at _Assignments_ &rarr; _Projects_ &rarr; _Project 2_ to submit the following:
    1. Your GitHub ID and Project 2 URL for grading.
    1. A short paragraph outlining (a) how you prompted the AI to generate any code (if you used it) and (b) how you determined the quality and correctness of that code.
    1. A screen shot showing no issues with `pre-commit run --all-files`.
  * Confirm on the GitHub Actions pane that the pass-off tests passed, or alternatively, goto the Project 2 URL, find the green checkmark or red x, and click it to confirm the auto-grader score matches the pass-off results from your system.

### Paragraph on AI

Guidelines for answering

_"A short paragraph outlining (a) how you prompted the AI to generate any code (if you used it) and (b) how you determined the quality and correctness of that code."_

These guidelines give examples from Project 1.

* Brief means no more than 500 words.
* Be specific about what code was generated. _"AI generated FSM code for the following non-terminals: `facts`, etc."_
* Be general about the final form of the prompts used to generate the code and any prompt iteration that was required. _"I gave the AI example code and asked it to create code that matched the pattern, and style, in that example code. I had to revise the prompt to specifically ask it to not generalize an FSM to detect a supplied string."_
* Be specific about how you determined the quality and correctness of generated code. _"A manual visual inspection was sufficient to determine quality and correctness because the generated code was trivial. I also ran the code quality tools on the generated code as a second level check."_
* Be specific about where else AI was leveraged. _"I used AI to breakdown and explain the pseudo-code for the `lexer` algorithm as well as the `FiniteStateMachine` class. AI also provided test inputs for my `STRING` FSM to help debug the apostrophe escape sequence."_
* Be specific about how you used AI to write tests, and how you generated the parse trees necessary for writing tests based on the math.

## Best Practices

Here is the suggested order for Project 2:

1. Run the tests in `test_datalogprogram.py` -- any tests associated with `DatalogProgram.__str__` should fail
1. Implement the `DatalogProgram.__str__` function so that the tests pass -- what it should output is described in [PARSER.md](docs/PARSER.md).
1. Write the grammar for a Datalog program.
1. For each production in the Datalog program grammar:

    1. Write two tests for the production in `./tests/test_parser.py` -- a test for bad input that checks for the `UnexpectedTokenException` exception and a test that returns the result of the production on a correct parse. For example, if you are working an the rule for `scheme` a correct parse should return an instance of `Predicate` that matches the defined scheme in the input file.
    1. Run the new tests -- they should fail
    1. Implement the production from the grammar -- done when the new tests pass

1. Run the pass-off tests -- debug as needed

The productions in the grammar can be implemented bottom-up -- starting with `id`, `id_list`, `scheme`, etc. -- or top down -- starting with `datalog_program`, `scheme`, `id`, `id_list`, etc. Bottom up may be easier though because `id` creates an instance of `str`, that is used by `id_list` to create a `list[Parameter]` instance, that is then used to create a `Predicate` for a `scheme`. In any case, work on, and test, productions one at a time.

If you use AI to generate productions, be sure it uses tail-recursion with `FOLLOW` sets for any list constructs, and be sure to give it adequate examples from code that you have written so that it follows the expected code structure for the project. The goal of this project is to learn how to implement a recursive descent parser. You should implement, and test, on your own several productions until you understand the pattern clearly. Once you have a solid understanding, then leverage AI to implement the more tedious, and simple, productions. As before, provide code examples for the AI to use.

### FAQ

#### Should I use a branch with git?

Consider using a branch as you work on your submission so that you can `commit` your work from time to time. Once everything is working, and the auto-grader tests are passing, then you can `merge` your work into your master branch and push it to your GitHub repository. Ask your favorite AI for help learning how to use Git branches for feature development.

#### How do I gather lists of objects while parsing?

Implementing grammar rules such as `schemeList` from the [Datalog Grammar](docs/PARSER.md#datalog-grammar) requires you to capture, and save, every `scheme` in the list. Recall that a `scheme` will be an instance of `Predicate` in our data structures. So how do you capture, and save, the `Predicate` instances from each match on the `scheme` rule.

First, the function for `scheme` should be `def scheme(token: TokenStream) -> Predicate` so that it returns the `Predicate` resulting from a successful parse of the `scheme` rule in the [Datalog Grammar](docs/PARSER.md#datalog-grammar). Note: matching the `scheme` production means that `token`, which is a `TokenStream`, has advanced past all the tokens used to match the `scheme` grammar rule. So `scheme(token)` returns a `Predicate` on success. But how do you collect each of these as you parse the `schemeList` production since that production is recursive?

You have two choices:

1. Side-effect on a passed in list and have the `scheme` function return nothing; or
1. Add the `Predicate` from the matched `scheme` to the returned list from the recursive call on the way out of recursion.

There are pros, and cons, to each approach.

The first solution becomes:

```
def scheme_list(token: TokenStream, pred_list: list[Predicate]) -> None
```

And is called as

```
pred_list: list[Predicate] = list()
scheme_list(token, pred_list)
```

The second solution becomes:

```
def scheme_list(token: TokenStream) -> list[Predicate]
```

And is called as:

```
pred_list: list[Predicate] = scheme_list(token, pred_list)
```

But here inside the implementation, where you have the recursive call, you need something like the below with the base case returning an empty list.

```
pred: Predicate = scheme(token)
return [Predicate] + scheme_list(token)
```

A con to the first approach is the side-effecting on the `pred_list` parameter whereas a con on the second approach is the copying of the list over and over again with the `+` operator. A hybrid approach is also possible where `scheme_list` returns `list[Predicate]` but the returned list is mutated on the way out of recursion. The concept of side-effecting was discussed in Homework 2, but feel free to use AI to help you understand what the term means in this context. Similarly, feel free to use AI to help you understand what is meant by "mutated on the way out of recursion".

#### What's a good way to organize your work on this project?

A good approach is to complete the project in three steps:

1. write a parser that only checks syntax -- this is the recursive-descent part. It does not build the final `DatalogProgram`.
1. add code to the parser to create the `DatalogProgram`. This can be done easily without modifying the lines of code that were created in the first step. For example when a parameter is being parsed a `Parameter` object is created and returned, then saved in the appropriate place.

#### What's a good way to handle syntax errors?

A good approach is to use exceptions to handle syntax errors. Throw an exception when the parser detects a syntax error. The `project2` function in `src/project2/project2.py` is already configured to catch the `UnexpectedTokenException` on a parse error and produce the correct output. **Don't return a boolean from each parsing routine to indicate a syntax error; just throw the `UnexpectedTokenException` with the offending token.

#### Why do you need the Parameter, Predicate, Rule, and Datalog Program classes, and what should be in these classes?

You need these classes because the next project builds on this project and these classes are needed for the next project. These new classes should not store tokens.

To determine what needs to be in these classes, use the code and the grammar as a guide. The code defines the class member variables. And in the grammar a predicate is defined as: ID LEFT_PAREN parameter parameterList RIGHT_PAREN. This means a predicate object needs to store a string (to hold the value of the initial ID) and a list of Parameter objects.


#### Should you store token types in the data structures created by the parser?

No, don't store tokens or token types in the data structures created by the parser. The data structures created by the parser should be decoupled from both the lexer and the parser as defined by the data structures in `src/project2/datalogprogram.py` that you must use.
